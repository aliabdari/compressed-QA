{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_compressed_domain_FE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliabdari/compressed-QA/blob/main/QA_compressed_domain_FE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg_WYNpyF4ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8b0a8a-63b7-424b-c352-fd74c5205608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.applications.vgg16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "model = VGG16(weights='imagenet', include_top=True)\n",
        "new_model = Model(model.inputs, model.layers[-2].output)"
      ],
      "metadata": {
        "id": "LeXP72GGhGfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frames(frames):\n",
        "    \n",
        "    # resized = cv2.resize(frame, (224,224), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "    # img_data = np.expand_dims(resized, axis=0)\n",
        "    # img_data = preprocess_input(frames)\n",
        "    features = new_model.predict(frames)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "Yw2cYqCsjlzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_vid(vid):\n",
        "    num_frame=0\n",
        "    cap= cv2.VideoCapture(vid)\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # feature_vector=np.zeros([2048,length])\n",
        "    frames = np.zeros([length,224,224,3])\n",
        "    while(cap.isOpened()):\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "             frames[num_frame] = preprocess_input(cv2.resize(frame, (224,224), interpolation = cv2.INTER_AREA))\n",
        "             num_frame += 1\n",
        "             if cv2.waitKey(30) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "    cap.release()\n",
        "    print(\"frames = \" + str(frames.shape))\n",
        "    features_transpose =  np.transpose(process_frames(frames =  frames))\n",
        "    print(\"features transpose = \" + str(features_transpose.shape))\n",
        "    return features_transpose"
      ],
      "metadata": {
        "id": "vOD29Drm2TYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def claculate_number_of_videos(path, extension):\n",
        "    num_vid = 0\n",
        "    for vid in glob.glob(path +'/*.' + extension):\n",
        "        num_vid+=1\n",
        "    return num_vid"
      ],
      "metadata": {
        "id": "AJSj8svX2XB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_path = '/content/drive/MyDrive/live_dataset_Reconstructed_Residuals'\n",
        "numVid = 0\n",
        "for c in glob.glob(classes_path + '/*'):\n",
        "  print(c)\n",
        "  numVid += claculate_number_of_videos(c, 'avi')\n",
        "print(\"number of videos which should be processed = \" + str(numVid))\n",
        "idx_vid = 0\n",
        "idx_class = 0\n",
        "x_train = np.empty(numVid,dtype=object)\n",
        "y_train = np.empty(numVid,dtype='int64')\n",
        "global x_train\n",
        "global y_train\n",
        "for c in glob.glob(classes_path + '/*'):\n",
        "  for vid in glob.glob(c + '/*.avi'):\n",
        "    #load every video of dataset\n",
        "    print(\"processing video number \" + str(idx_vid +1) +\" of \" + str(numVid) + \" started\")\n",
        "    current_fv = process_vid(vid)\n",
        "    x_train[idx_vid] = current_fv\n",
        "    y_train[idx_vid] = np.array([idx_class])\n",
        "    idx_vid+=1\n",
        "    print(str(idx_vid) + \" of \" + str(numVid) + \" video is processed\")\n",
        "  idx_class += 1\n",
        "print(\"producing dataset finished\")"
      ],
      "metadata": {
        "id": "fAr_MjXs2eyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/information_live/x_vgg16',x_train)"
      ],
      "metadata": {
        "id": "nQWQgpJw3fsw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}